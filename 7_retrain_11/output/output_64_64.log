nohup: 忽略输入
{'n_cpu': 0, 'device': device(type='cuda', index=1), 'batch_size_test': 40, 'batch_size': 2, 'lr': 0.001, 'weight_decay': 0, 'display_interval': 250, 'num_epochs': 50, 'early_stopping': True, 'patience': 5, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'input_dim': 8, 'output_dim': 2, 'input_length': 12, 'output_length': 24, 'input_gap': 1, 'pred_shift': 24, 'model_i': 0, 'kernel_size': (3, 3), 'bias': True, 'hidden_dim_1': (64, 64, 64, 64), 'd_attn_1': 64, 'ssr_decay_rate': 8e-05, 'hidden_dim_2': (32, 32, 32, 32), 'd_attn_2': 16, 'hidden_dim_3': (32, 32, 32, 32), 'd_attn_3': 16, 'hidden_dim_4': (32, 32, 32, 32), 'd_attn_4': 16, 'hidden_dim_5': (32, 32, 32, 32), 'd_attn_5': 16, 'use_hc': 1, 'time': 'hybrid', 'save_last': 0}

reading data
(1692, 24, 48, 21, 4) (432, 24, 48, 1, 4)
(1692, 21, 3) (432, 1, 3)
processing training set
(151, 38, 24, 48, 21, 4)
(3171, 38, 24, 48, 4)
(3171, 38, 3)
predData.shape= (3171, 38, 2, 24, 48)
{'sst': (3171, 38, 24, 48, 6), 'nino target': (3171, 38, 3)}
processing eval set
(395, 38, 24, 48, 1, 4)
(395, 38, 24, 48, 4)
(395, 38, 3)
predData.shape= (395, 38, 2, 24, 48)
{'sst': (395, 38, 24, 48, 6), 'nino target': (395, 38, 3)}
Total params num: 2071938
*****************Finish Parameter****************
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 959.73, 83.03, score: -14.6456, ssr ratio: 0.9999
batch training loss: 434.90, 31.92, score: 71.5304, ssr ratio: 0.9799
batch training loss: 505.10, 43.57, score: 96.1298, ssr ratio: 0.9599
batch training loss: 407.76, 22.35, score: 92.8247, ssr ratio: 0.9399
batch training loss: 467.11, 20.61, score: 106.2404, ssr ratio: 0.9199
batch training loss: 395.47, 21.29, score: 106.2508, ssr ratio: 0.8999
batch training loss: 398.82, 16.42, score: 106.3238, ssr ratio: 0.8799
tensor([0.9318, 0.8802, 0.8357, 0.8017, 0.7759, 0.7507, 0.7262, 0.7034, 0.6805,
        0.6570, 0.6396, 0.6323, 0.6347, 0.6403, 0.6422, 0.6354, 0.6158, 0.5861,
        0.5599, 0.5452, 0.5351, 0.5299, 0.5305, 0.5319], device='cuda:1')
epoch eval loss:
sst: 648.49, nino: 127.30, sc: 48.7823
eval score is improved from -inf to 48.78234, saving model

epoch: 2
batch training loss: 481.53, 19.07, score: 73.6452, ssr ratio: 0.8730
batch training loss: 399.58, 32.46, score: 88.5389, ssr ratio: 0.8530
batch training loss: 390.68, 13.48, score: 107.1774, ssr ratio: 0.8330
batch training loss: 420.60, 18.11, score: 83.6976, ssr ratio: 0.8130
batch training loss: 438.48, 51.56, score: 101.5999, ssr ratio: 0.7930
batch training loss: 442.32, 35.24, score: 70.8380, ssr ratio: 0.7730
batch training loss: 482.07, 18.36, score: 106.0229, ssr ratio: 0.7530
tensor([0.9342, 0.8837, 0.8391, 0.8039, 0.7774, 0.7540, 0.7326, 0.7126, 0.6913,
        0.6655, 0.6408, 0.6248, 0.6215, 0.6247, 0.6254, 0.6172, 0.5958, 0.5654,
        0.5395, 0.5255, 0.5153, 0.5090, 0.5077, 0.5080], device='cuda:1')
epoch eval loss:
sst: 603.98, nino: 122.92, sc: 47.7912
Epoch 00002: reducing learning rate of group 0 to 3.0000e-04.
eval score is not improved for 1 epoch

epoch: 3
batch training loss: 468.84, 20.81, score: 105.5365, ssr ratio: 0.7462
batch training loss: 412.37, 18.91, score: 91.9099, ssr ratio: 0.7262
batch training loss: 440.17, 23.63, score: 61.9503, ssr ratio: 0.7062
batch training loss: 423.17, 28.90, score: 104.9420, ssr ratio: 0.6862
batch training loss: 414.44, 16.95, score: 73.4824, ssr ratio: 0.6662
batch training loss: 390.87, 22.77, score: 105.4159, ssr ratio: 0.6462
batch training loss: 420.81, 24.44, score: 106.0357, ssr ratio: 0.6262
tensor([0.9359, 0.8843, 0.8365, 0.7969, 0.7647, 0.7359, 0.7115, 0.6914, 0.6719,
        0.6489, 0.6269, 0.6109, 0.6030, 0.5990, 0.5928, 0.5789, 0.5539, 0.5228,
        0.4979, 0.4855, 0.4781, 0.4739, 0.4725, 0.4731], device='cuda:1')
epoch eval loss:
sst: 592.51, nino: 125.33, sc: 44.2000
Epoch 00003: reducing learning rate of group 0 to 1.0000e-04.
eval score is not improved for 2 epoch

epoch: 4
batch training loss: 406.13, 20.42, score: 101.7188, ssr ratio: 0.6193
batch training loss: 424.00, 15.98, score: 98.8327, ssr ratio: 0.5993
batch training loss: 434.15, 21.88, score: 105.5573, ssr ratio: 0.5793
batch training loss: 416.51, 20.58, score: 95.3264, ssr ratio: 0.5593
batch training loss: 414.73, 26.22, score: 105.6966, ssr ratio: 0.5393
batch training loss: 404.05, 19.06, score: 105.9535, ssr ratio: 0.5193
batch training loss: 465.85, 34.49, score: 101.9274, ssr ratio: 0.4993
tensor([0.9415, 0.8965, 0.8571, 0.8265, 0.8028, 0.7792, 0.7554, 0.7308, 0.7038,
        0.6738, 0.6481, 0.6324, 0.6283, 0.6289, 0.6265, 0.6138, 0.5880, 0.5554,
        0.5297, 0.5180, 0.5122, 0.5118, 0.5161, 0.5211], device='cuda:1')
epoch eval loss:
sst: 588.93, nino: 122.71, sc: 48.2294
eval score is not improved for 3 epoch

epoch: 5
batch training loss: 465.37, 32.18, score: 104.0443, ssr ratio: 0.4924
batch training loss: 451.49, 25.67, score: 105.3064, ssr ratio: 0.4724
batch training loss: 453.27, 33.77, score: 94.7508, ssr ratio: 0.4524
batch training loss: 448.70, 26.93, score: 88.8864, ssr ratio: 0.4324
batch training loss: 529.84, 25.16, score: 94.1230, ssr ratio: 0.4124
batch training loss: 499.15, 28.53, score: 91.9244, ssr ratio: 0.3924
batch training loss: 415.19, 48.65, score: 102.2921, ssr ratio: 0.3724
tensor([0.9398, 0.8936, 0.8532, 0.8219, 0.7986, 0.7764, 0.7542, 0.7309, 0.7052,
        0.6768, 0.6524, 0.6370, 0.6317, 0.6306, 0.6265, 0.6120, 0.5841, 0.5487,
        0.5191, 0.5031, 0.4936, 0.4901, 0.4917, 0.4954], device='cuda:1')
epoch eval loss:
sst: 587.09, nino: 122.83, sc: 47.2939
eval score is not improved for 4 epoch

epoch: 6
batch training loss: 520.01, 26.02, score: 67.4964, ssr ratio: 0.3655
batch training loss: 466.68, 57.59, score: 102.5756, ssr ratio: 0.3455
batch training loss: 555.14, 27.34, score: 103.5935, ssr ratio: 0.3255
batch training loss: 477.26, 11.51, score: 97.0981, ssr ratio: 0.3055
batch training loss: 456.69, 29.51, score: 78.8422, ssr ratio: 0.2855
batch training loss: 507.14, 70.91, score: 100.1017, ssr ratio: 0.2655
batch training loss: 434.64, 36.01, score: 91.7868, ssr ratio: 0.2455
tensor([0.9407, 0.8951, 0.8538, 0.8203, 0.7930, 0.7671, 0.7439, 0.7226, 0.7004,
        0.6751, 0.6528, 0.6388, 0.6342, 0.6329, 0.6281, 0.6133, 0.5856, 0.5511,
        0.5233, 0.5092, 0.5008, 0.4973, 0.4985, 0.5021], device='cuda:1')
epoch eval loss:
sst: 589.66, nino: 121.81, sc: 47.7928
eval score is not improved for 5 epoch
early stopping reached, best score is 48.782338

----- training finished -----

processing test set
(395, 38, 24, 48, 1, 4)
(395, 38, 24, 48, 4)
(395, 38, 3)
predData.shape= (395, 38, 2, 24, 48)
{'sst': (395, 38, 24, 48, 6), 'nino target': (395, 38, 3)}
loading test dataloader
Traceback (most recent call last):
  File "trainer.py", line 264, in <module>
    trainer.network.load_state_dict(chk['net'])
  File "/home/ruichuang/anaconda3/envs/AGCRN_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1604, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for SAConvLSTM:
	size mismatch for layers.0.conv.weight: copying a param with shape torch.Size([512, 136, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 72, 3, 3]).
	size mismatch for layers.0.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.0.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 64, 1, 1]).
	size mismatch for layers.0.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).
	size mismatch for layers.0.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 128, 3, 3]).
	size mismatch for layers.0.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for layers.1.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).
	size mismatch for layers.1.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.1.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 64, 1, 1]).
	size mismatch for layers.1.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).
	size mismatch for layers.1.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 128, 3, 3]).
	size mismatch for layers.1.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for layers.2.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).
	size mismatch for layers.2.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.2.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 64, 1, 1]).
	size mismatch for layers.2.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).
	size mismatch for layers.2.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 128, 3, 3]).
	size mismatch for layers.2.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for layers.3.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).
	size mismatch for layers.3.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
	size mismatch for layers.3.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 64, 1, 1]).
	size mismatch for layers.3.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).
	size mismatch for layers.3.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 128, 3, 3]).
	size mismatch for layers.3.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for conv_output.weight: copying a param with shape torch.Size([2, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 64, 1, 1]).
