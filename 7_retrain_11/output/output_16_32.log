nohup: 忽略输入
{'n_cpu': 0, 'device': device(type='cuda', index=4), 'batch_size_test': 40, 'batch_size': 2, 'lr': 0.001, 'weight_decay': 0, 'display_interval': 250, 'num_epochs': 50, 'early_stopping': True, 'patience': 5, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'input_dim': 8, 'output_dim': 2, 'input_length': 12, 'output_length': 24, 'input_gap': 1, 'pred_shift': 24, 'model_i': 0, 'kernel_size': (3, 3), 'bias': True, 'hidden_dim_1': (32, 32, 32, 32), 'd_attn_1': 16, 'ssr_decay_rate': 8e-05, 'hidden_dim_2': (32, 32, 32, 32), 'd_attn_2': 16, 'hidden_dim_3': (32, 32, 32, 32), 'd_attn_3': 16, 'hidden_dim_4': (32, 32, 32, 32), 'd_attn_4': 16, 'hidden_dim_5': (32, 32, 32, 32), 'd_attn_5': 16, 'use_hc': 1, 'time': 'hybrid', 'save_last': 0}

reading data
(1692, 24, 48, 21, 4) (432, 24, 48, 1, 4)
(1692, 21, 3) (432, 1, 3)
processing training set
(151, 38, 24, 48, 21, 4)
(3171, 38, 24, 48, 4)
(3171, 38, 3)
predData.shape= (3171, 38, 2, 24, 48)
{'sst': (3171, 38, 24, 48, 6), 'nino target': (3171, 38, 3)}
processing eval set
(395, 38, 24, 48, 1, 4)
(395, 38, 24, 48, 4)
(395, 38, 3)
predData.shape= (395, 38, 2, 24, 48)
{'sst': (395, 38, 24, 48, 6), 'nino target': (395, 38, 3)}
Total params num: 465218
*****************Finish Parameter****************
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 1111.55, 123.01, score: -32.2894, ssr ratio: 0.9999
batch training loss: 464.99, 44.62, score: 48.4659, ssr ratio: 0.9799
batch training loss: 529.18, 39.77, score: 84.9757, ssr ratio: 0.9599
batch training loss: 419.33, 25.31, score: 92.3830, ssr ratio: 0.9399
batch training loss: 481.89, 20.27, score: 106.2709, ssr ratio: 0.9199
batch training loss: 402.49, 26.81, score: 93.9170, ssr ratio: 0.8999
batch training loss: 405.38, 14.34, score: 106.6786, ssr ratio: 0.8799
tensor([0.9284, 0.8706, 0.8151, 0.7653, 0.7209, 0.6806, 0.6457, 0.6166, 0.5912,
        0.5675, 0.5524, 0.5521, 0.5652, 0.5828, 0.5944, 0.5941, 0.5788, 0.5516,
        0.5257, 0.5071, 0.4897, 0.4751, 0.4650, 0.4553], device='cuda:4')
epoch eval loss:
sst: 673.42, nino: 130.64, sc: 42.0630
eval score is improved from -inf to 42.06298, saving model

epoch: 2
batch training loss: 487.43, 21.90, score: 73.3285, ssr ratio: 0.8730
batch training loss: 408.11, 35.44, score: 87.8661, ssr ratio: 0.8530
batch training loss: 399.10, 15.94, score: 106.6434, ssr ratio: 0.8330
batch training loss: 421.83, 18.23, score: 77.8182, ssr ratio: 0.8130
batch training loss: 441.84, 53.51, score: 101.3103, ssr ratio: 0.7930
batch training loss: 445.83, 34.53, score: 70.8930, ssr ratio: 0.7730
batch training loss: 489.64, 23.86, score: 105.4951, ssr ratio: 0.7530
tensor([0.9403, 0.8952, 0.8546, 0.8216, 0.7959, 0.7717, 0.7491, 0.7280, 0.7074,
        0.6857, 0.6674, 0.6569, 0.6549, 0.6556, 0.6521, 0.6391, 0.6134, 0.5798,
        0.5503, 0.5309, 0.5148, 0.5011, 0.4910, 0.4833], device='cuda:4')
epoch eval loss:
sst: 668.37, nino: 125.34, sc: 49.0124
eval score is improved from 42.06298 to 49.01243, saving model

epoch: 3
batch training loss: 470.77, 22.71, score: 89.1816, ssr ratio: 0.7462
batch training loss: 420.60, 25.66, score: 91.1804, ssr ratio: 0.7262
batch training loss: 448.46, 20.24, score: 78.6378, ssr ratio: 0.7062
batch training loss: 440.67, 64.82, score: 99.4007, ssr ratio: 0.6862
batch training loss: 425.46, 30.21, score: 71.5293, ssr ratio: 0.6662
batch training loss: 400.39, 29.38, score: 104.3628, ssr ratio: 0.6462
batch training loss: 428.18, 24.29, score: 90.0576, ssr ratio: 0.6262
tensor([0.9366, 0.8883, 0.8460, 0.8146, 0.7929, 0.7731, 0.7548, 0.7368, 0.7165,
        0.6914, 0.6680, 0.6524, 0.6467, 0.6452, 0.6410, 0.6286, 0.6058, 0.5768,
        0.5539, 0.5434, 0.5373, 0.5349, 0.5367, 0.5397], device='cuda:4')
epoch eval loss:
sst: 615.35, nino: 126.24, sc: 49.8248
eval score is improved from 49.01243 to 49.82478, saving model

epoch: 4
batch training loss: 410.84, 19.78, score: 101.7884, ssr ratio: 0.6193
batch training loss: 432.86, 13.98, score: 96.1578, ssr ratio: 0.5993
batch training loss: 439.01, 25.29, score: 104.8366, ssr ratio: 0.5793
batch training loss: 425.59, 26.02, score: 94.7078, ssr ratio: 0.5593
batch training loss: 419.05, 24.34, score: 105.9887, ssr ratio: 0.5393
batch training loss: 410.79, 23.98, score: 105.3391, ssr ratio: 0.5193
batch training loss: 476.81, 34.74, score: 101.6925, ssr ratio: 0.4993
tensor([0.9398, 0.8966, 0.8589, 0.8301, 0.8097, 0.7914, 0.7742, 0.7564, 0.7357,
        0.7111, 0.6886, 0.6741, 0.6692, 0.6680, 0.6636, 0.6502, 0.6262, 0.5971,
        0.5742, 0.5630, 0.5560, 0.5531, 0.5542, 0.5568], device='cuda:4')
epoch eval loss:
sst: 601.91, nino: 117.03, sc: 53.1039
eval score is improved from 49.82478 to 53.10390, saving model

epoch: 5
batch training loss: 471.55, 26.54, score: 104.5434, ssr ratio: 0.4924
batch training loss: 458.03, 34.96, score: 104.3490, ssr ratio: 0.4724
batch training loss: 467.02, 41.09, score: 93.6299, ssr ratio: 0.4524
batch training loss: 460.23, 33.24, score: 88.7774, ssr ratio: 0.4324
batch training loss: 542.08, 33.18, score: 93.3580, ssr ratio: 0.4124
batch training loss: 505.31, 30.24, score: 91.6831, ssr ratio: 0.3924
batch training loss: 424.04, 41.11, score: 103.2198, ssr ratio: 0.3724
tensor([0.9405, 0.8961, 0.8558, 0.8235, 0.7985, 0.7754, 0.7534, 0.7317, 0.7089,
        0.6836, 0.6615, 0.6480, 0.6451, 0.6466, 0.6446, 0.6327, 0.6086, 0.5768,
        0.5498, 0.5348, 0.5245, 0.5193, 0.5199, 0.5228], device='cuda:4')
epoch eval loss:
sst: 592.73, nino: 120.37, sc: 49.8734
Epoch 00005: reducing learning rate of group 0 to 3.0000e-04.
eval score is not improved for 1 epoch

epoch: 6
batch training loss: 527.39, 25.89, score: 78.3074, ssr ratio: 0.3655
batch training loss: 470.69, 57.49, score: 102.7609, ssr ratio: 0.3455
batch training loss: 557.71, 32.32, score: 103.2506, ssr ratio: 0.3255
batch training loss: 478.90, 13.70, score: 96.7359, ssr ratio: 0.3055
batch training loss: 460.76, 32.33, score: 82.9790, ssr ratio: 0.2855
batch training loss: 513.87, 71.23, score: 99.7954, ssr ratio: 0.2655
batch training loss: 436.67, 37.81, score: 91.5773, ssr ratio: 0.2455
tensor([0.9369, 0.8903, 0.8479, 0.8132, 0.7851, 0.7597, 0.7380, 0.7189, 0.6993,
        0.6765, 0.6559, 0.6423, 0.6376, 0.6364, 0.6323, 0.6186, 0.5930, 0.5608,
        0.5347, 0.5214, 0.5134, 0.5098, 0.5108, 0.5142], device='cuda:4')
epoch eval loss:
sst: 590.46, nino: 121.26, sc: 48.5921
Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.
eval score is not improved for 2 epoch

epoch: 7
batch training loss: 443.91, 54.96, score: 46.3333, ssr ratio: 0.2386
batch training loss: 418.03, 44.41, score: 103.8201, ssr ratio: 0.2186
batch training loss: 435.76, 32.09, score: 88.1275, ssr ratio: 0.1986
batch training loss: 531.57, 56.15, score: -11.5770, ssr ratio: 0.1786
batch training loss: 499.41, 23.92, score: 94.3838, ssr ratio: 0.1586
batch training loss: 517.62, 23.77, score: 89.6225, ssr ratio: 0.1386
batch training loss: 503.68, 22.69, score: 93.9869, ssr ratio: 0.1186
tensor([0.9371, 0.8902, 0.8479, 0.8129, 0.7834, 0.7549, 0.7291, 0.7061, 0.6833,
        0.6583, 0.6369, 0.6238, 0.6208, 0.6216, 0.6185, 0.6044, 0.5777, 0.5453,
        0.5209, 0.5104, 0.5047, 0.5024, 0.5043, 0.5081], device='cuda:4')
epoch eval loss:
sst: 588.81, nino: 122.75, sc: 47.0158
eval score is not improved for 3 epoch

epoch: 8
batch training loss: 504.81, 25.00, score: 105.2703, ssr ratio: 0.1118
batch training loss: 560.08, 47.42, score: 36.4008, ssr ratio: 0.0918
batch training loss: 573.86, 73.97, score: 74.3805, ssr ratio: 0.0718
batch training loss: 607.05, 60.99, score: 81.5091, ssr ratio: 0.0518
batch training loss: 473.82, 26.74, score: 40.5051, ssr ratio: 0.0318
batch training loss: 569.25, 53.21, score: 73.0308, ssr ratio: 0.0118
batch training loss: 524.45, 19.78, score: 88.1657, ssr ratio: 0.0000
tensor([0.9351, 0.8868, 0.8417, 0.8034, 0.7706, 0.7396, 0.7128, 0.6905, 0.6694,
        0.6459, 0.6258, 0.6138, 0.6123, 0.6149, 0.6140, 0.6023, 0.5773, 0.5455,
        0.5214, 0.5108, 0.5047, 0.5024, 0.5048, 0.5095], device='cuda:4')
epoch eval loss:
sst: 590.27, nino: 123.40, sc: 46.4854
eval score is not improved for 4 epoch

epoch: 9
batch training loss: 519.78, 93.49, score: 97.8514, ssr ratio: 0.0000
batch training loss: 552.58, 38.08, score: 90.8211, ssr ratio: 0.0000
batch training loss: 498.64, 34.42, score: 96.8771, ssr ratio: 0.0000
batch training loss: 567.66, 54.74, score: 101.7283, ssr ratio: 0.0000
batch training loss: 554.94, 47.87, score: 64.3554, ssr ratio: 0.0000
batch training loss: 533.17, 21.62, score: 97.8801, ssr ratio: 0.0000
batch training loss: 522.60, 51.36, score: 100.7487, ssr ratio: 0.0000
tensor([0.9347, 0.8869, 0.8430, 0.8058, 0.7737, 0.7425, 0.7154, 0.6926, 0.6714,
        0.6477, 0.6280, 0.6169, 0.6171, 0.6211, 0.6212, 0.6096, 0.5847, 0.5535,
        0.5303, 0.5208, 0.5154, 0.5134, 0.5159, 0.5206], device='cuda:4')
epoch eval loss:
sst: 590.80, nino: 123.78, sc: 47.2352
eval score is not improved for 5 epoch
early stopping reached, best score is 53.103899

----- training finished -----

processing test set
(395, 38, 24, 48, 1, 4)
(395, 38, 24, 48, 4)
(395, 38, 3)
predData.shape= (395, 38, 2, 24, 48)
{'sst': (395, 38, 24, 48, 6), 'nino target': (395, 38, 3)}
loading test dataloader
Traceback (most recent call last):
  File "trainer.py", line 264, in <module>
    trainer.network.load_state_dict(chk['net'])
  File "/home/ruichuang/anaconda3/envs/AGCRN_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1604, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for SAConvLSTM:
	size mismatch for layers.0.conv.weight: copying a param with shape torch.Size([512, 136, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 40, 3, 3]).
	size mismatch for layers.0.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 32, 1, 1]).
	size mismatch for layers.0.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([48]).
	size mismatch for layers.0.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 1, 1]).
	size mismatch for layers.0.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.0.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 32, 1, 1]).
	size mismatch for layers.0.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for layers.0.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).
	size mismatch for layers.0.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.1.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).
	size mismatch for layers.1.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 32, 1, 1]).
	size mismatch for layers.1.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([48]).
	size mismatch for layers.1.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 1, 1]).
	size mismatch for layers.1.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.1.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 32, 1, 1]).
	size mismatch for layers.1.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for layers.1.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).
	size mismatch for layers.1.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.2.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).
	size mismatch for layers.2.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 32, 1, 1]).
	size mismatch for layers.2.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([48]).
	size mismatch for layers.2.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 1, 1]).
	size mismatch for layers.2.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.2.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 32, 1, 1]).
	size mismatch for layers.2.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for layers.2.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).
	size mismatch for layers.2.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.3.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).
	size mismatch for layers.3.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 32, 1, 1]).
	size mismatch for layers.3.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([48]).
	size mismatch for layers.3.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 1, 1]).
	size mismatch for layers.3.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.3.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 32, 1, 1]).
	size mismatch for layers.3.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for layers.3.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).
	size mismatch for layers.3.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for conv_output.weight: copying a param with shape torch.Size([2, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 32, 1, 1]).
