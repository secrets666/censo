nohup: 忽略输入
{'n_cpu': 0, 'device': device(type='cuda', index=6), 'batch_size_test': 40, 'batch_size': 2, 'lr': 0.001, 'weight_decay': 0, 'display_interval': 250, 'num_epochs': 50, 'early_stopping': True, 'patience': 5, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'input_dim': 8, 'output_dim': 2, 'input_length': 12, 'output_length': 24, 'input_gap': 1, 'pred_shift': 24, 'model_i': 0, 'kernel_size': (3, 3), 'bias': True, 'hidden_dim_1': (32, 32, 32, 32), 'd_attn_1': 32, 'ssr_decay_rate': 8e-05, 'hidden_dim_2': (32, 32, 32, 32), 'd_attn_2': 16, 'hidden_dim_3': (32, 32, 32, 32), 'd_attn_3': 16, 'hidden_dim_4': (32, 32, 32, 32), 'd_attn_4': 16, 'hidden_dim_5': (32, 32, 32, 32), 'd_attn_5': 16, 'use_hc': 1, 'time': 'hybrid', 'save_last': 0}

reading data
(1692, 24, 48, 21, 4) (432, 24, 48, 1, 4)
(1692, 21, 3) (432, 1, 3)
processing training set
(151, 38, 24, 48, 21, 4)
(3171, 38, 24, 48, 4)
(3171, 38, 3)
predData.shape= (3171, 38, 2, 24, 48)
{'sst': (3171, 38, 24, 48, 6), 'nino target': (3171, 38, 3)}
processing eval set
(395, 38, 24, 48, 1, 4)
(395, 38, 24, 48, 4)
(395, 38, 3)
predData.shape= (395, 38, 2, 24, 48)
{'sst': (395, 38, 24, 48, 6), 'nino target': (395, 38, 3)}
Total params num: 537282
*****************Finish Parameter****************
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 1049.15, 114.04, score: -8.4938, ssr ratio: 0.9999
batch training loss: 462.21, 38.75, score: 65.8919, ssr ratio: 0.9799
batch training loss: 521.44, 45.00, score: 84.4608, ssr ratio: 0.9599
batch training loss: 417.93, 16.66, score: 93.1981, ssr ratio: 0.9399
batch training loss: 478.25, 20.05, score: 106.3139, ssr ratio: 0.9199
batch training loss: 410.40, 24.57, score: 94.3140, ssr ratio: 0.8999
batch training loss: 403.40, 15.45, score: 106.4291, ssr ratio: 0.8799
tensor([0.9323, 0.8791, 0.8312, 0.7913, 0.7583, 0.7278, 0.7002, 0.6767, 0.6549,
        0.6327, 0.6163, 0.6112, 0.6165, 0.6245, 0.6267, 0.6197, 0.6013, 0.5743,
        0.5505, 0.5368, 0.5259, 0.5193, 0.5179, 0.5171], device='cuda:6')
epoch eval loss:
sst: 626.82, nino: 132.65, sc: 46.4201
eval score is improved from -inf to 46.42009, saving model

epoch: 2
batch training loss: 486.52, 20.98, score: 73.3259, ssr ratio: 0.8730
batch training loss: 409.42, 38.46, score: 87.4189, ssr ratio: 0.8530
batch training loss: 400.57, 15.86, score: 106.7511, ssr ratio: 0.8330
batch training loss: 424.84, 20.79, score: 77.3415, ssr ratio: 0.8130
batch training loss: 441.74, 50.34, score: 101.6103, ssr ratio: 0.7930
batch training loss: 443.53, 34.40, score: 70.7933, ssr ratio: 0.7730
batch training loss: 489.58, 23.04, score: 105.3726, ssr ratio: 0.7530
tensor([0.9330, 0.8826, 0.8389, 0.8062, 0.7830, 0.7611, 0.7380, 0.7136, 0.6884,
        0.6624, 0.6423, 0.6328, 0.6343, 0.6395, 0.6397, 0.6295, 0.6064, 0.5751,
        0.5483, 0.5328, 0.5215, 0.5143, 0.5120, 0.5107], device='cuda:6')
epoch eval loss:
sst: 606.68, nino: 121.75, sc: 48.6211
eval score is improved from 46.42009 to 48.62111, saving model

epoch: 3
batch training loss: 471.91, 21.11, score: 105.4028, ssr ratio: 0.7462
batch training loss: 421.16, 28.08, score: 91.1088, ssr ratio: 0.7262
batch training loss: 446.11, 24.32, score: 61.6960, ssr ratio: 0.7062
batch training loss: 437.69, 56.40, score: 100.7705, ssr ratio: 0.6862
batch training loss: 419.60, 17.54, score: 89.2254, ssr ratio: 0.6662
batch training loss: 401.83, 29.08, score: 104.6536, ssr ratio: 0.6462
batch training loss: 428.53, 24.11, score: 90.0965, ssr ratio: 0.6262
tensor([0.9388, 0.8920, 0.8509, 0.8197, 0.7973, 0.7768, 0.7574, 0.7392, 0.7205,
        0.6982, 0.6768, 0.6618, 0.6557, 0.6535, 0.6484, 0.6356, 0.6130, 0.5844,
        0.5612, 0.5498, 0.5426, 0.5387, 0.5382, 0.5383], device='cuda:6')
epoch eval loss:
sst: 620.13, nino: 123.10, sc: 50.9528
eval score is improved from 48.62111 to 50.95281, saving model

epoch: 4
batch training loss: 411.41, 15.15, score: 102.1618, ssr ratio: 0.6193
batch training loss: 431.50, 13.41, score: 99.0763, ssr ratio: 0.5993
batch training loss: 440.27, 22.92, score: 105.3266, ssr ratio: 0.5793
batch training loss: 423.61, 24.58, score: 94.9445, ssr ratio: 0.5593
batch training loss: 420.55, 26.06, score: 105.9289, ssr ratio: 0.5393
batch training loss: 409.77, 21.49, score: 100.4149, ssr ratio: 0.5193
batch training loss: 473.68, 32.51, score: 101.9489, ssr ratio: 0.4993
tensor([0.9432, 0.9020, 0.8682, 0.8439, 0.8256, 0.8062, 0.7851, 0.7621, 0.7372,
        0.7110, 0.6897, 0.6774, 0.6742, 0.6735, 0.6694, 0.6567, 0.6334, 0.6042,
        0.5802, 0.5675, 0.5586, 0.5541, 0.5545, 0.5555], device='cuda:6')
epoch eval loss:
sst: 599.57, nino: 116.51, sc: 53.7274
eval score is improved from 50.95281 to 53.72742, saving model

epoch: 5
batch training loss: 471.43, 27.32, score: 104.5353, ssr ratio: 0.4924
batch training loss: 459.52, 37.57, score: 104.1236, ssr ratio: 0.4724
batch training loss: 465.99, 40.01, score: 93.8131, ssr ratio: 0.4524
batch training loss: 458.45, 29.22, score: 82.9005, ssr ratio: 0.4324
batch training loss: 541.47, 34.33, score: 81.8718, ssr ratio: 0.4124
batch training loss: 505.03, 32.03, score: 91.1088, ssr ratio: 0.3924
batch training loss: 424.10, 42.35, score: 102.9977, ssr ratio: 0.3724
tensor([0.9417, 0.8979, 0.8603, 0.8323, 0.8110, 0.7892, 0.7671, 0.7448, 0.7214,
        0.6964, 0.6763, 0.6654, 0.6644, 0.6665, 0.6642, 0.6512, 0.6250, 0.5908,
        0.5626, 0.5477, 0.5381, 0.5341, 0.5354, 0.5374], device='cuda:6')
epoch eval loss:
sst: 592.29, nino: 119.40, sc: 51.5884
Epoch 00005: reducing learning rate of group 0 to 3.0000e-04.
eval score is not improved for 1 epoch

epoch: 6
batch training loss: 528.59, 31.03, score: 66.7389, ssr ratio: 0.3655
batch training loss: 469.76, 54.71, score: 103.0073, ssr ratio: 0.3455
batch training loss: 558.66, 35.02, score: 102.8477, ssr ratio: 0.3255
batch training loss: 478.19, 14.81, score: 101.8134, ssr ratio: 0.3055
batch training loss: 463.04, 35.26, score: 92.9192, ssr ratio: 0.2855
batch training loss: 514.74, 77.54, score: 82.2799, ssr ratio: 0.2655
batch training loss: 436.43, 36.92, score: 91.5837, ssr ratio: 0.2455
tensor([0.9409, 0.8960, 0.8559, 0.8227, 0.7946, 0.7678, 0.7436, 0.7218, 0.7005,
        0.6778, 0.6592, 0.6485, 0.6465, 0.6466, 0.6422, 0.6278, 0.6013, 0.5683,
        0.5415, 0.5273, 0.5184, 0.5140, 0.5142, 0.5164], device='cuda:6')
epoch eval loss:
sst: 591.25, nino: 120.32, sc: 49.4070
Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.
eval score is not improved for 2 epoch

epoch: 7
batch training loss: 444.52, 52.79, score: 63.0977, ssr ratio: 0.2386
batch training loss: 418.03, 43.67, score: 103.8343, ssr ratio: 0.2186
batch training loss: 435.25, 30.71, score: 88.4192, ssr ratio: 0.1986
batch training loss: 529.95, 55.27, score: -11.2421, ssr ratio: 0.1786
batch training loss: 498.88, 25.07, score: 94.2144, ssr ratio: 0.1586
batch training loss: 517.18, 23.38, score: 85.4440, ssr ratio: 0.1386
batch training loss: 502.59, 25.35, score: 104.9860, ssr ratio: 0.1186
tensor([0.9413, 0.8957, 0.8552, 0.8214, 0.7912, 0.7602, 0.7313, 0.7056, 0.6809,
        0.6552, 0.6343, 0.6226, 0.6214, 0.6235, 0.6213, 0.6079, 0.5809, 0.5472,
        0.5210, 0.5088, 0.5019, 0.4993, 0.5007, 0.5034], device='cuda:6')
epoch eval loss:
sst: 588.87, nino: 122.60, sc: 47.0618
eval score is not improved for 3 epoch

epoch: 8
batch training loss: 503.06, 26.06, score: 104.9562, ssr ratio: 0.1118
batch training loss: 559.95, 47.71, score: 36.1701, ssr ratio: 0.0918
batch training loss: 573.79, 72.91, score: 74.4292, ssr ratio: 0.0718
batch training loss: 607.24, 58.23, score: 91.7810, ssr ratio: 0.0518
batch training loss: 473.37, 24.82, score: 30.9707, ssr ratio: 0.0318
batch training loss: 568.66, 54.19, score: 72.8858, ssr ratio: 0.0118
batch training loss: 524.24, 19.22, score: 88.2053, ssr ratio: 0.0000
tensor([0.9384, 0.8914, 0.8488, 0.8122, 0.7790, 0.7463, 0.7177, 0.6938, 0.6717,
        0.6487, 0.6303, 0.6205, 0.6206, 0.6234, 0.6214, 0.6081, 0.5811, 0.5471,
        0.5207, 0.5081, 0.5011, 0.4990, 0.5015, 0.5059], device='cuda:6')
epoch eval loss:
sst: 590.16, nino: 123.09, sc: 46.7778
eval score is not improved for 4 epoch

epoch: 9
batch training loss: 520.21, 94.67, score: 97.7778, ssr ratio: 0.0000
batch training loss: 552.54, 37.48, score: 90.8121, ssr ratio: 0.0000
batch training loss: 499.69, 35.81, score: 96.6164, ssr ratio: 0.0000
batch training loss: 567.41, 51.15, score: 102.1028, ssr ratio: 0.0000
batch training loss: 554.14, 49.31, score: 64.1759, ssr ratio: 0.0000
batch training loss: 534.26, 22.52, score: 97.6120, ssr ratio: 0.0000
batch training loss: 523.55, 50.67, score: 100.7603, ssr ratio: 0.0000
tensor([0.9378, 0.8902, 0.8469, 0.8096, 0.7761, 0.7432, 0.7146, 0.6909, 0.6690,
        0.6461, 0.6283, 0.6192, 0.6203, 0.6239, 0.6224, 0.6093, 0.5826, 0.5494,
        0.5241, 0.5125, 0.5061, 0.5043, 0.5071, 0.5114], device='cuda:6')
epoch eval loss:
sst: 590.19, nino: 123.99, sc: 46.8454
eval score is not improved for 5 epoch
early stopping reached, best score is 53.727421

----- training finished -----

processing test set
(395, 38, 24, 48, 1, 4)
(395, 38, 24, 48, 4)
(395, 38, 3)
predData.shape= (395, 38, 2, 24, 48)
{'sst': (395, 38, 24, 48, 6), 'nino target': (395, 38, 3)}
loading test dataloader
Traceback (most recent call last):
  File "trainer.py", line 264, in <module>
    trainer.network.load_state_dict(chk['net'])
  File "/home/ruichuang/anaconda3/envs/AGCRN_env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1604, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for SAConvLSTM:
	size mismatch for layers.0.conv.weight: copying a param with shape torch.Size([512, 136, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 40, 3, 3]).
	size mismatch for layers.0.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.0.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 32, 1, 1]).
	size mismatch for layers.0.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.0.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1]).
	size mismatch for layers.0.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.0.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).
	size mismatch for layers.0.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.0.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 64, 3, 3]).
	size mismatch for layers.0.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.1.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).
	size mismatch for layers.1.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.1.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 32, 1, 1]).
	size mismatch for layers.1.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.1.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1]).
	size mismatch for layers.1.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.1.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).
	size mismatch for layers.1.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.1.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 64, 3, 3]).
	size mismatch for layers.1.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.2.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).
	size mismatch for layers.2.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.2.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 32, 1, 1]).
	size mismatch for layers.2.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.2.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1]).
	size mismatch for layers.2.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.2.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).
	size mismatch for layers.2.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.2.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 64, 3, 3]).
	size mismatch for layers.2.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.3.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).
	size mismatch for layers.3.conv.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for layers.3.sa.conv_h.weight: copying a param with shape torch.Size([192, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 32, 1, 1]).
	size mismatch for layers.3.sa.conv_h.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for layers.3.sa.conv_m.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1]).
	size mismatch for layers.3.sa.conv_m.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for layers.3.sa.conv_z.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 64, 1, 1]).
	size mismatch for layers.3.sa.conv_z.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for layers.3.sa.conv_output.weight: copying a param with shape torch.Size([384, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 64, 3, 3]).
	size mismatch for layers.3.sa.conv_output.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for conv_output.weight: copying a param with shape torch.Size([2, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 32, 1, 1]).
